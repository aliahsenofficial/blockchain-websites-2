{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(projects, new_scraped_writer, all_scraped_writer):\n",
    "    for project in projects:\n",
    "        data = {}\n",
    "\n",
    "        data['Report'] = project.get_attribute('href')\n",
    "\n",
    "        if data['Report'] not in previous_reports:\n",
    "            try:\n",
    "                data['Name'] = project.find_element(By.CSS_SELECTOR, 'div.space-name.g-ellipsis').text.strip()\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            data['Website'] = ''\n",
    "            \n",
    "            data['Telegram'] = ''\n",
    "\n",
    "            data['Twitter'] = ''\n",
    "\n",
    "            data['Discord'] = ''\n",
    "\n",
    "            new_scraped_writer.writerow(data)\n",
    "            all_scraped_writer.writerow(data)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reports_from_csv():\n",
    "    df = pd.read_csv(\"All_Scraped.csv\")\n",
    "    report_list = df[\"Report\"].tolist()\n",
    "    return report_list\n",
    "\n",
    "previous_reports = read_reports_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "# options.add_argument('--headless=new')\n",
    "# options.add_argument('--ignore-certificate-errors')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "url = 'https://taskon.xyz/space'\n",
    "\n",
    "driver.get(url)\n",
    "element = WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.CSS_SELECTOR, 'a.space-item-view')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_to_end():\n",
    "    last_project = driver.find_elements(By.CSS_SELECTOR, 'a.space-item-view')[-1]\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", last_project)\n",
    "\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> Scrolling Started\")\n",
    "i = 1\n",
    "projects = []\n",
    "while True:\n",
    "    if len(projects) == len(driver.find_elements(By.CSS_SELECTOR, 'a.space-item-view')):\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            scroll_to_end()\n",
    "            projects.extend(driver.find_elements(By.CSS_SELECTOR, 'a.space-item-view'))\n",
    "        except:\n",
    "            break\n",
    "        print('Scrolled: ', i)\n",
    "        i+=1\n",
    "\n",
    "print(\">> Scrolling Completed\")\n",
    "\n",
    "# projects = driver.find_elements(By.CSS_SELECTOR, 'a.space-item-view')\n",
    "print('>> Total Projects: ', len(projects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Name','Report','Website','Telegram','Twitter','Discord']\n",
    "with open(\"All_Scraped.csv\", mode='a', newline='', encoding='utf-8') as all_scraped_file, open(\"New_Scraped.csv\", mode='a', newline='', encoding='utf-8') as new_scraped_file:\n",
    "    new_scraped_writer = csv.DictWriter(new_scraped_file, fieldnames=header)\n",
    "    all_scraped_writer = csv.DictWriter(all_scraped_file, fieldnames=header)\n",
    "        \n",
    "    #     print('Writing to file...')\n",
    "    print('>> Project Scraping Started')\n",
    "    for i, project in enumerate(tqdm(projects, desc='Projects Scraping')):\n",
    "        data = {}\n",
    "\n",
    "        data['Report'] = project.get_attribute('href')\n",
    "\n",
    "        if data['Report'] not in previous_reports:\n",
    "            data['Name'] = project.text.split('\\n')[0]\n",
    "            data['Website'] = ''\n",
    "            data['Telegram'] = ''\n",
    "            data['Twitter'] = ''\n",
    "            data['Discord'] = ''\n",
    "\n",
    "            new_scraped_writer.writerow(data)\n",
    "            all_scraped_writer.writerow(data)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = driver.find_elements(By.CSS_SELECTOR, 'a.space-item-view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project in projects:\n",
    "    print(project.text.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
