{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reports_from_csv():\n",
    "    df = pd.read_csv(\"All_Scraped.csv\")\n",
    "    report_list = df[\"Report\"].tolist()\n",
    "    return report_list\n",
    "\n",
    "previous_reports = read_reports_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(projects, new_scraped_writer, all_scraped_writer):\n",
    "    for project in projects:\n",
    "        print(project)\n",
    "        if project not in previous_reports:\n",
    "            data = {}\n",
    "\n",
    "            project_response = requests.get(project)\n",
    "            project_soup = BeautifulSoup(project_response.content, 'lxml')\n",
    "\n",
    "            data['Report'] = project\n",
    "            try:\n",
    "                data['Name'] = project_soup.select_one('h1.airdrop__title').text.split('(')[0].strip()\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                data['Website'] = project_soup.select_one('a[title=\"Go to airdrops website for more information\"]').get('href')\n",
    "            except:\n",
    "                data['Website'] = ''\n",
    "                \n",
    "            try:\n",
    "                data['Twitter'] = project_soup.select_one('a[title=\"Go to the airdrops Twitter\"]').get('href')\n",
    "            except:\n",
    "                data['Twitter'] = ''\n",
    "\n",
    "            try:\n",
    "                data['Telegram'] = project_soup.select_one('a[title=\"Join the airdrops Telegram\"]').get('href')\n",
    "            except:\n",
    "                data['Telegram'] = ''\n",
    "\n",
    "            data['Discord'] = ''\n",
    "\n",
    "            new_scraped_writer.writerow(data)\n",
    "            all_scraped_writer.writerow(data)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Name','Website','Telegram','Twitter','Discord','Report']\n",
    "with open(\"All_Scraped.csv\", mode='a', newline='', encoding='utf-8') as all_scraped_file, open(\"New_Scraped.csv\", mode='a', newline='', encoding='utf-8') as new_scraped_file:\n",
    "    new_scraped_writer = csv.DictWriter(new_scraped_file, fieldnames=header)\n",
    "    all_scraped_writer = csv.DictWriter(all_scraped_file, fieldnames=header)\n",
    "\n",
    "    links = [   'https://airdropalert.com/defi-airdrops', \n",
    "                'https://airdropalert.com/nft-airdrops',\n",
    "                'https://airdropalert.com/new-airdrops',\n",
    "                'https://airdropalert.com/featured-airdrops',\n",
    "                'https://airdropalert.com/upcoming-airdrops',\n",
    "                'https://airdropalert.com/past-airdrops'\n",
    "            ]\n",
    "\n",
    "    projects = []\n",
    "    for link in links[:2]:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        total_pages = int(soup.select_one('section.paging-section > div > ul > li:nth-child(12) > a').text)\n",
    "\n",
    "        # for i in range(2, total_pages+2):\n",
    "        for i in range(2, 3):\n",
    "            print(\"Page: \", i)\n",
    "            for project in soup.select('div.card.shadow.text-center > a'):\n",
    "                projects.append(project.get('href'))\n",
    "                \n",
    "            print(len(projects))\n",
    "            if i < (total_pages+1):\n",
    "                response = requests.get(link + f'?page={i}')\n",
    "                soup = BeautifulSoup(response.content, 'lxml')\n",
    "    \n",
    "    get_data(list(set(projects[:10])), new_scraped_writer, all_scraped_writer)\n",
    "    print('All Data Saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
